#!/usr/bin/env python3

import numpy as np
import json
import ampwrapper.utils as amputils
from ampwrapper.fit import FitResults
import argparse
import sys
from pathlib import Path
import shutil
import re
from tqdm import tqdm
import pandas as pd
import subprocess
import time

def main():
    env_path = amputils.get_environment()
    parser = argparse.ArgumentParser()
    red_queue = {"cpu": 1990, "threads": 4}
    green_queue = {"cpu": 1590, "threads": 4}
    blue_queue = {"cpu": 1990, "threads": 4}
    queues = {"red": red_queue, "green": green_queue, "blue": blue_queue}

    with open(env_path, 'r') as env_file:
        env = json.load(env_file)
    if not env.get('studies'):
        print(amputils.wrap("You must initialize at least one AmpTools study using amptools-study!"))
        sys.exit(1)
    config_keys = list(amputils.get_configs().keys())
    study_keys = list(env['studies'].keys())
    parser.add_argument("-s", "--study", choices=study_keys, help="name of AmpTools study to bootstrap")
    parser.add_argument("-c", "--config", choices=config_keys, help="name of AmpTools config to use in bootstrap")
    parser.add_argument("-i", "--iterations", type=int, default=1, help="number of fits to do for each bin (randomized initializations)")
    parser.add_argument("-a", "--append", action="store_true", help="append these iterations to any existing fits rather than rerunning")
    parser.add_argument("--seed", default=1, help="seed for randomization")
    parser.add_argument("--skip-fit", action="store_true", help="skip fitting and just collect available results from any previous fits")
    parser.add_argument("-q", "--queue", choices=["red", "green", "blue"], default="blue", help="SLURM queue for jobs")
    parser.add_argument("--no-mem", action="store_true", help="don't set a memory cap in the SLURM script (use for large bins where the generated MC is a huge file")
    parser.add_argument("--time-limit", action="store_true", help="add 4-hour time limit to SLURM job")
    # Bootstrap specific
    parser.add_argument("--no-data", action="store_true", help="(optional) skip bootstrapping on the data (and background, if applicable) file(s)")
    parser.add_argument("--gen", action="store_true", help="(optional) bootstrap generated Monte Carlo")
    parser.add_argument("--acc", action="store_true", help="(optional) bootstrap accepted Monte Carlo")
    args = parser.parse_args()
    if args.no_data:
        if not (args.gen or args.acc):
            print(amputils.wrap("You must select at least one bootstrapping option between --gen or --acc if you choose --no-data!"))
            sys.exit(1)
    queue = queues[args.queue]

    # Validation
    args.study, args.config = amputils.get_study_config(args.study, args.config)

    flags = ""
    if args.no_data:
        flags += "_nodata"
    if args.gen:
        flags += "_gen"
    if args.acc:
        flags += "_acc"
    print(amputils.DEFAULT(f"Initializing AmpTools bootstrapping on study {args.study} using {args.config} as the fit configuration"))
    study = env['studies'][args.study]
    res_file = Path(study['directory']) / f"{args.config}_results.csv"
    fit_dir = Path(study['directory']) / f"{args.config}_bootstrap{flags}"
    fit_dir.mkdir(exist_ok=True)
    slurm_path = Path(study['directory']) / f"dispatch_{args.config}_bootstrap{flags}.csh"
    if not res_file.exists():
        print(amputils.wrap("This configuration has not yet been fit for this study, run amptools-fit first!"))
        sys.exit(1)
    # Get the best fit in each bin
    df = pd.read_csv(res_file)
    bin_dfs = [df.loc[df['bin'] == i_bin] for i_bin in range(study['nbins'])]
    best_df = pd.concat([bin_df[bin_df['likelihood'] == bin_df['likelihood'].min()] for bin_df in bin_dfs])
    best_df = best_df.drop_duplicates(subset=['bin'])
    # Make sure there *is* a best fit in each bin
    if best_df:
        missing_bins = set(range(study['nbins'])) - set(best_df['bin'])
        if missing_bins:
            print(amputils.wrap(f"Missing fits for the following bins: {missing_bins}. Currently, this program will not run without a fit in each bin."))
            sys.exit(1)
    else:
        print(amputils.wrap("Failed to find best iteration in any bin, make sure results have been collected!"))
        sys.exit(1)
    # Make directories
    iterations = list(range(args.iterations))
    for i_bin in range(study['nbins']):
        bin_path = fit_dir / str(i_bin)
        bin_path.mkdir(exist_ok=True)
        if args.append:
            existing_iteration_nums = [int(path.name) for path in bin_path.iterdir()]
            if existing_iteration_nums:
                max_it = max(existing_iteration_nums)
            else:
                max_it = -1
            iterations = list(range(max_it + 1, max_it + 1 + args.iterations))
        best_fit_iteration = int(best_df.loc[best_df['bin'] == i_bin]['iteration'])
        for i_it in iterations:
            np.random.seed(int(args.seed) + i_it)
            it_path = bin_path / str(i_it)
            it_path.mkdir(exist_ok=True)
            # Copy configuration file
            config_path = amputils.get_configs()[args.config]
            config_it_path = it_path / f"{args.config}_{i_bin}-{i_it}.cfg"
            shutil.copy(config_path, config_it_path)
            # Change data reader to a bootstrap reader with a seed
            with open(config_it_path, 'r') as config_file:
                config_text = config_file.read()
                if not args.no_data:
                    bootstrap_seed = np.random.randint(100000)
                    config_text = re.sub(r"data\s(\w+)\sROOTDataReader\sLOOPDATAFILE",
                                        rf"data \1 ROOTDataReaderBootstrap LOOPDATAFILE {bootstrap_seed}",
                                        config_text)
                    bootstrap_seed = np.random.randint(100000)
                    config_text = re.sub(r"bkgnd\s(\w+)\sROOTDataReader\sLOOPBKGFILE",
                                         rf"bkgnd \1 ROOTDataReaderBootstrap LOOPBKGFILE {bootstrap_seed}",
                                         config_text)
                if args.gen:
                    bootstrap_seed = np.random.randint(100000)
                    config_text = re.sub(r"genmc\s(\w+)\sROOTDataReader\sLOOPGENFILE",
                                         rf"genmc \1 ROOTDataReaderBootstrap LOOPGENFILE {bootstrap_seed}",
                                         config_text)
                if args.acc:
                    bootstrap_seed = np.random.randint(100000)
                    config_text = re.sub(r"accmc\s(\w+)\sROOTDataReader\sLOOPACCFILE",
                                         rf"accmc \1 ROOTDataReaderBootstrap LOOPACCFILE {bootstrap_seed}",
                                         config_text)
                best_fit = best_df.loc[best_df['bin'] == i_bin].loc[best_df['iteration'] == best_fit_iteration].to_dict(orient='records')[0]
                init_re = re.compile(r"(initialize \w+::\w+::)(AMP\S+) polar (@uniform @polaruniform|@uniform 0\.0)(\n| real\n)")
                found_all = False
                while not found_all:
                    match = init_re.search(config_text)
                    if match:
                        value = complex(best_fit.get(match.group(2) + "@amp"))
                        original = match.group(0)
                        replacement = f"{match.group(1)}{match.group(2)} cartesian {np.real(value)} {np.imag(value)} {match.group(4)}"
                        config_text = config_text.replace(original, replacement)
                    else:
                        found_all = True
                init_params_re = re.compile(r"(?:^|\n)(parameter \w+) ([-+]?[0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)")
                # starts with newline to allow for commenting out parameters without errors
                for param_name, placeholder_value in init_params_re.findall(config_text):
                    fit_value = best_fit.get(f"{param_name}@par")
                    config_text.replace(f"parameter {param_name} {placeholder_value}", f"parameter {param_name} {fit_value}")
                p = re.compile(r"@(\w*)_(\w*)")
                matches = p.findall(config_text)
                for match in matches:
                    file_tag = {"AMO": "AMO", "000": "PARA_0", "045": "PERP_45", "090": "PERP_90", "135": "PARA_135"}.get(match[1])
                    if not file_tag:
                        print(amputils.wrap("Error in parsing configuration file tags!"))
                        sys.exit(1)
                    file_paths = [file_path for file_path in (Path(study['directory']) / match[0]).iterdir() if file_tag in str(file_path.name) and file_path.stem.endswith(f"_{i_bin}")]
                    if not file_paths:
                        print(amputils.wrap(f"Error locating the {match[0]} file with polarization {file_tag} for bin {i_bin}!"))
                        sys.exit(1)
                    if len(file_paths) > 1:
                        print(amputils.wrap(f"Warning: More than one file matches {file_tag} for {match[0]} in bin {i_bin}!"))
                    config_text = config_text.replace(f"@{match[0]}_{match[1]}", str(file_paths[0]))

            with open(config_it_path, 'w') as config_file:
                config_file.write(config_text)
    # Dispatch jobs
    with open(slurm_path, 'w') as slurm_file:
        lines = ["#!/bin/tcsh -f\n"]
        lines.append(f"#SBATCH --ntasks={queue['threads']}\n")
        lines.append(f"#SBATCH --partition={args.queue}\n")
        if not args.no_mem:
            lines.append(f"#SBATCH --mem={queue['threads'] * queue['cpu']}\n")
        if args.time_limit:
            lines.append("#SBATCH --time=4:00:00\n")
        lines.append(f"#SBATCH --output={study['directory']}/{args.config}/log_{args.config}_bootstrap_%A.log\n")
        lines.append("#SBATCH --quiet\n")
        lines.append(f"#SBATCH --array={iterations[0]}-{iterations[-1]}\n")
        lines.append("pwd; hostname; date; whoami\n")
        lines.append("echo $SLURM_ARRAY_TASK_ID\n")
        lines.append(f"cd {study['directory']}/{args.config}_bootstrap{flags}/$1/$SLURM_ARRAY_TASK_ID\n")
        lines.append(f"pwd\n")
        lines.append(f"fit -c {args.config}_${{1}}-${{SLURM_ARRAY_TASK_ID}}.cfg\n")
        lines.append("echo DONE!; date")
        slurm_file.writelines(lines)

    # Run fits
    if not args.skip_fit:
        job_names = []
        for i_bin in tqdm(range(study['nbins'])):
            subprocess.run(["sbatch", "-J", f"{args.config}_{str(i_bin)}", str(slurm_path), str(i_bin)])
            job_names.append(f"{args.config}_{i_bin}")
        # Wait for all jobs to finish running
        running = True
        while running:
            n_jobs_running, n_jobs_in_queue = amputils.check_SLURM(job_names)
            print(f"{n_jobs_in_queue:4} job(s) in queue | {n_jobs_running:4} job(s) running", end="\r")
            if n_jobs_in_queue == 0:
                running = False
            time.sleep(2) # don't check it so often
        print()

    # Collect results
    res_path = Path(study['directory']) / f"{args.config}_results_bootstrap{flags}.csv"
    df = pd.DataFrame()
    for i_bin in tqdm(range(study['nbins'])):
        bin_path = fit_dir / str(i_bin)
        for it in [int(path.name) for path in bin_path.iterdir()]:
            fit_path = Path(study['directory']) / f"{args.config}_bootstrap{flags}/{i_bin}/{it}/{amputils.get_config_reaction(args.config)}.fit"
            if not fit_path.exists():
                print(f"No fit file found for bin {i_bin} iteration {it}")
                continue
            wrapper = FitResults.FitResultsWrapper(str(fit_path))
            amp_list = [s.decode().split("::", 1)[1] for s in wrapper.ampList()]
            polarizations = [f"_{pol}" for pol in amputils.get_config_pols(args.config)]
            res_dict = {"bin": i_bin, "iteration": it}
            for amp in amp_list:
                if 'Re' in amp:
                    wave = amp.split("::")[-1]
                    wave_set = [f"{amputils.get_config_reaction(args.config)}{pol}::{amp}" for pol in polarizations]
                    wave_set.extend([f"{amputils.get_config_reaction(args.config)}{pol}::{amp.replace('Re', 'Im')}" for pol in polarizations])
                    wave_pol0 = f"{amputils.get_config_reaction(args.config)}{polarizations[0]}::{amp}"
                    res_dict[f"{wave}@int"], res_dict[f"{wave}@int@err"] = wrapper.intensity(wave_set, False)
                    res_dict[f"{wave}@int@acc"], res_dict[f"{wave}@int@acc@err"] = wrapper.intensity(wave_set, True)
                    res_dict[f"{wave}@amp"] = wrapper.productionParameter(wave_pol0)
            res_dict["total@int"], res_dict["total@int@err"] = wrapper.total_intensity(False)
            res_dict["total@int@acc"], res_dict["total@int@acc@err"] = wrapper.total_intensity(True)
            res_dict["likelihood"] = wrapper.likelihood()
            """
            amp_pairs = combinations(amp_list, 2)
            for amp1, amp2 in amp_pairs:
                wave1 = amp1.split("::")[-1]
                wave2 = amp2.split("::")[-1]
                res_dict[f"{wave1}::{wave2}@phase"], res_dict[f"{wave1}::{wave2}@phase@err"] = wrapper.phaseDiff(amp1, amp2)
            """
            df = pd.concat([pd.DataFrame(res_dict, index=[0]), df], ignore_index=True)
    df.to_csv(res_path, index=False)
    if not study.get('bootstraps'):
        study['bootstraps'] = []
    if not args.config in study['bootstraps']:
        study['bootstraps'].append(args.config)
    with open(env_path, 'w') as env_file:
        json.dump(env, env_file, indent=4)



if __name__ == "__main__":
    main()
